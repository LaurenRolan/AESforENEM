{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498700fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import os \n",
    "\n",
    "prompts_paths = glob.glob(\".\\\\prompts\\\\*\\\\*.txt\")\n",
    "essays_paths = glob.glob(\".\\\\prompts\\\\*\\\\*\\\\*.txt\")\n",
    "marks_paths = glob.glob(\".\\\\prompts\\\\*\\\\*\\\\*.csv\")\n",
    "\n",
    "df = pd.DataFrame(columns=['prompt', 'essay', 'mark1', 'mark2', 'mark3', 'mark4', 'mark5'])\n",
    "\n",
    "for prompt_path in prompts_paths:\n",
    "    prompt_number = re.search(\"(\\d+)\", prompt_path).group(0)\n",
    "    with open(prompt_path, 'r') as f:\n",
    "        prompt = f.read()\n",
    "        find1 = re.findall(\"(.*?)Como enviar sua redação\", prompt)\n",
    "        find2 = re.findall(\"(.*?)ObservaçõesSeu\", prompt)\n",
    "        if find1:\n",
    "            prompt = find1[0]\n",
    "        if find2:\n",
    "            prompt = find2[0]\n",
    "    essays = []\n",
    "    marks = pd.DataFrame({0: [], 1:[], 2:[], 3:[], 4:[]})\n",
    "    for essay_path in essays_paths:\n",
    "        #Get essays for this prompt, and the original one\n",
    "        if \"prompt\"+str(prompt_number)+\"\\\\\" in essay_path and \"original\" in essay_path:\n",
    "            with open(essay_path, \"r\", encoding='utf-8') as original:\n",
    "                essay = original.read()\n",
    "                essays.append(essay)\n",
    "            mark_path = essay_path.replace(\"_original.txt\", \"_mark.csv\")\n",
    "            mark = pd.read_csv(mark_path).transpose().drop(axis=0, index=\"Topics\").drop(columns=[5])\n",
    "            marks = pd.concat([marks, mark], ignore_index=True)\n",
    "    prompts = [prompt] * len(essays)\n",
    "    sub_df = pd.DataFrame({'prompt': prompts, 'essay': essays, 'mark1': marks[0], 'mark2':marks[1], 'mark3':marks[2], 'mark4':marks[3], 'mark5':marks[4]})\n",
    "    df = pd.concat([df, sub_df], ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9c1f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>essay</th>\n",
       "      <th>mark1</th>\n",
       "      <th>mark2</th>\n",
       "      <th>mark3</th>\n",
       "      <th>mark4</th>\n",
       "      <th>mark5</th>\n",
       "      <th>marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O número de pessoas desempregadas no mundo dev...</td>\n",
       "      <td>Qualificações para o mercado de trabalho\\n\\nAn...</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>[80, 120, 120, 80, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O número de pessoas desempregadas no mundo dev...</td>\n",
       "      <td>Futuro da crise do desemprego\\n\\nA Qualificaçã...</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>[80, 80, 80, 80, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O número de pessoas desempregadas no mundo dev...</td>\n",
       "      <td>O progresso da tecnologia\\n\\nNo decorrer dos s...</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>[160, 160, 160, 160, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O número de pessoas desempregadas no mundo dev...</td>\n",
       "      <td>O advento tecnológico \\n\\nAs constantes mudanç...</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>[160, 120, 120, 120, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O número de pessoas desempregadas no mundo dev...</td>\n",
       "      <td>Transformações laborais\\n\\nA noção de emprego,...</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>[120, 120, 120, 120, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Reportagem publicada pelo UOL Economia no mês ...</td>\n",
       "      <td>Vencer na vida\\n\\nProlongada pela ONU em 1948,...</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>[120, 80, 20, 20, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Reportagem publicada pelo UOL Economia no mês ...</td>\n",
       "      <td>Vencer ou ser?\\n\\nVencer é algo que desde cedo...</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>[160, 160, 120, 160, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Reportagem publicada pelo UOL Economia no mês ...</td>\n",
       "      <td>“E essa é a vitória que vence o mundo: a nossa...</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>[160, 120, 160, 160, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Reportagem publicada pelo UOL Economia no mês ...</td>\n",
       "      <td>O que é mais importante para vencer na vida?\\n...</td>\n",
       "      <td>20</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>[20, 80, 20, 20, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Reportagem publicada pelo UOL Economia no mês ...</td>\n",
       "      <td>Fatores mais importantes para vencer na vida\\n...</td>\n",
       "      <td>120</td>\n",
       "      <td>20</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>[120, 20, 80, 80, 80]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    O número de pessoas desempregadas no mundo dev...   \n",
       "1    O número de pessoas desempregadas no mundo dev...   \n",
       "2    O número de pessoas desempregadas no mundo dev...   \n",
       "3    O número de pessoas desempregadas no mundo dev...   \n",
       "4    O número de pessoas desempregadas no mundo dev...   \n",
       "..                                                 ...   \n",
       "451  Reportagem publicada pelo UOL Economia no mês ...   \n",
       "452  Reportagem publicada pelo UOL Economia no mês ...   \n",
       "453  Reportagem publicada pelo UOL Economia no mês ...   \n",
       "454  Reportagem publicada pelo UOL Economia no mês ...   \n",
       "455  Reportagem publicada pelo UOL Economia no mês ...   \n",
       "\n",
       "                                                 essay mark1 mark2 mark3  \\\n",
       "0    Qualificações para o mercado de trabalho\\n\\nAn...    80   120   120   \n",
       "1    Futuro da crise do desemprego\\n\\nA Qualificaçã...    80    80    80   \n",
       "2    O progresso da tecnologia\\n\\nNo decorrer dos s...   160   160   160   \n",
       "3    O advento tecnológico \\n\\nAs constantes mudanç...   160   120   120   \n",
       "4    Transformações laborais\\n\\nA noção de emprego,...   120   120   120   \n",
       "..                                                 ...   ...   ...   ...   \n",
       "451  Vencer na vida\\n\\nProlongada pela ONU em 1948,...   120    80    20   \n",
       "452  Vencer ou ser?\\n\\nVencer é algo que desde cedo...   160   160   120   \n",
       "453  “E essa é a vitória que vence o mundo: a nossa...   160   120   160   \n",
       "454  O que é mais importante para vencer na vida?\\n...    20    80    20   \n",
       "455  Fatores mais importantes para vencer na vida\\n...   120    20    80   \n",
       "\n",
       "    mark4 mark5                      marks  \n",
       "0      80    80     [80, 120, 120, 80, 80]  \n",
       "1      80    80       [80, 80, 80, 80, 80]  \n",
       "2     160   120  [160, 160, 160, 160, 120]  \n",
       "3     120   120  [160, 120, 120, 120, 120]  \n",
       "4     120   120  [120, 120, 120, 120, 120]  \n",
       "..    ...   ...                        ...  \n",
       "451    20     0       [120, 80, 20, 20, 0]  \n",
       "452   160   120  [160, 160, 120, 160, 120]  \n",
       "453   160   120  [160, 120, 160, 160, 120]  \n",
       "454    20    20       [20, 80, 20, 20, 20]  \n",
       "455    80    80      [120, 20, 80, 80, 80]  \n",
       "\n",
       "[456 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_list(values):\n",
    "    print(values.tolist())\n",
    "df['marks'] = df[df.columns[2:7]].values.astype(int).tolist()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd21dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "326460d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'essay', 'mark1', 'mark2', 'mark3', 'mark4', 'mark5', 'marks', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 364\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'essay', 'mark1', 'mark2', 'mark3', 'mark4', 'mark5', 'marks', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 46\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['prompt', 'essay', 'mark1', 'mark2', 'mark3', 'mark4', 'mark5', 'marks', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 46\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('adalbertojunior/distilbert-portuguese-cased')\n",
    "model = BertModel.from_pretrained(\"adalbertojunior/distilbert-portuguese-cased\", output_attentions=True)#.to(\"cuda\")\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "train_testvalid = ds.train_test_split(test_size=0.2,seed=15)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5,seed=15)\n",
    "\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"essay\"], truncation=True,max_length=512)\n",
    "\n",
    "tokenized_dataset = data.map(tokenize, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cea37e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"marks\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b55f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = model = BertModel.from_pretrained(\"adalbertojunior/distilbert-portuguese-cased\",output_attentions=True,output_hidden_states=True)\n",
    "        for param in model.parameters():\n",
    "          param.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier1_mark1 = nn.Linear(768, 64) # Relates to the language domain\n",
    "        self.classifier1_mark2 = nn.Linear(768, 64) # Relates to the understanding of the prompt\n",
    "        self.classifier1_mark3 = nn.Linear(768, 64) # Relates to the argument organization\n",
    "        self.classifier1_mark4 = nn.Linear(768, 64) # Relates to the liguistic mechanisms of argumentative texts\n",
    "        self.classifier1_mark5 = nn.Linear(768, 64) # Relates to the solution elaboration\n",
    "\n",
    "        self.classifier2_mark1 = nn.Linear(64, 1) # Relates to the language domain\n",
    "        self.classifier2_mark2 = nn.Linear(64, 1) # Relates to the understanding of the prompt\n",
    "        self.classifier2_mark3 = nn.Linear(64, 1) # Relates to the argument organization\n",
    "        self.classifier2_mark4 = nn.Linear(64, 1) # Relates to the liguistic mechanisms of argumentative texts\n",
    "        self.classifier2_mark5 = nn.Linear(64, 1) # Relates to the solution elaboration\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,marks=None):\n",
    "        #Extract outputs from the body\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #Add custom layers\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "        logit_mark1 = self.classifier1_mark1(sequence_output[:,0,:].view(-1,768))\n",
    "        logit_mark2 = self.classifier1_mark2(sequence_output[:,0,:].view(-1,768))\n",
    "        logit_mark3 = self.classifier1_mark3(sequence_output[:,0,:].view(-1,768))\n",
    "        logit_mark4 = self.classifier1_mark4(sequence_output[:,0,:].view(-1,768))\n",
    "        logit_mark5 = self.classifier1_mark5(sequence_output[:,0,:].view(-1,768))\n",
    "\n",
    "        logit_mark1 = F.relu(logit_mark1)\n",
    "        logit_mark2 = F.relu(logit_mark2)\n",
    "        logit_mark3 = F.relu(logit_mark3)\n",
    "        logit_mark4 = F.relu(logit_mark4)\n",
    "        logit_mark5 = F.relu(logit_mark5)\n",
    "\n",
    "        logit_mark1 = self.dropout1(logit_mark1)\n",
    "        logit_mark2 = self.dropout2(logit_mark2)\n",
    "        logit_mark3 = self.dropout3(logit_mark3)\n",
    "        logit_mark4 = self.dropout4(logit_mark4)\n",
    "        logit_mark5 = self.dropout5(logit_mark5)\n",
    "\n",
    "        logit_mark1 = self.classifier2_mark1(logit_mark1)\n",
    "        logit_mark2 = self.classifier2_mark2(logit_mark2)\n",
    "        logit_mark3 = self.classifier2_mark3(logit_mark3)\n",
    "        logit_mark4 = self.classifier2_mark4(logit_mark4)\n",
    "        logit_mark5 = self.classifier2_mark5(logit_mark5)\n",
    "\n",
    "        logits = torch.cat((logit_mark1, logit_mark2, logit_mark3, logit_mark4, logit_mark5), 1)\n",
    "\n",
    "        loss1 = None\n",
    "        loss2 = None\n",
    "        loss3 = None\n",
    "        loss4 = None\n",
    "        loss5 = None\n",
    "        if marks is not None:\n",
    "            loss_fct = nn.MSELoss()\n",
    "#            loss = loss_fct(logits.view(-1, self.num_labels), marks)\n",
    "            loss1 = loss_fct(logit_mark1, marks[:,0].view(-1,1))\n",
    "            loss2 = loss_fct(logit_mark2, marks[:,1].view(-1,1))\n",
    "            loss3 = loss_fct(logit_mark3, marks[:,2].view(-1,1))\n",
    "            loss4 = loss_fct(logit_mark4, marks[:,3].view(-1,1))\n",
    "            loss5 = loss_fct(logit_mark5, marks[:,4].view(-1,1))\n",
    "\n",
    "        return dict(loss1=loss1, loss2=loss2, loss3=loss3, loss4=loss4, loss5=loss5,\n",
    "                    logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f796736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device = torch.device(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=CustomModel(num_labels=5).to(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"valid\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba692645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "194c529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "metric = load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3eba4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938ceb149505469ebeff052685568d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1c710291334ffe82fcc655156f9850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 768])\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     12\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[37], line 25\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[1;34m(self, input_ids, attention_mask, marks)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m marks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TokenClassifierOutput(loss\u001b[38;5;241m=\u001b[39mloss, logits\u001b[38;5;241m=\u001b[39mlogits, hidden_states\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mhidden_states,attentions\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mattentions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "#import os\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:128\"\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "metrics = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss1 = outputs['loss1']\n",
    "        loss2 = outputs['loss2']\n",
    "        loss3 = outputs['loss3']\n",
    "        loss4 = outputs['loss4']\n",
    "        loss5 = outputs['loss5']\n",
    "\n",
    "        loss = loss1 + loss2 + loss3 + loss4 + loss5\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs['logits']\n",
    "        metric.update(preds=logits, target=batch[\"marks\"])\n",
    "        progress_bar_eval.update(1)\n",
    "\n",
    "    metrics.append(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224a159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def report_gpu():\n",
    "    #print(torch.cuda.list_gpu_processes())\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec9eb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5784e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cpu_metrics = torch.tensor(metrics, device = 'cpu')\n",
    "plt.plot(cpu_metrics)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
